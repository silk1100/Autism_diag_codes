{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from joblib import load\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMinMax:\n",
    "    def __init__(self, axis):\n",
    "        self.sc = MinMaxScaler()\n",
    "        self.axis = axis\n",
    "\n",
    "    def fit(self, X):\n",
    "        if self.axis==1:\n",
    "            self.sc = self.sc.fit(X.transpose())\n",
    "        elif self.axis==0:\n",
    "            self.sc = self.sc.fit(X)\n",
    "        return self.sc\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.axis==1:\n",
    "            Xn = self.sc.transform(X.transpose()).transpose()\n",
    "        elif self.axis==0:\n",
    "            Xn = self.sc.transform(X)\n",
    "        return Xn\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        if self.axis==1:\n",
    "            self.sc = self.sc.fit(X.transpose())\n",
    "            Xn = self.sc.transform(X.transpose()).transpose()\n",
    "        elif self.axis==0:\n",
    "            self.sc = self.sc.fit(X)\n",
    "            Xn = self.sc.transform(X)\n",
    "        return Xn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mynormalize(df, allfeats=False):\n",
    "    scalersdict = {}\n",
    "    if allfeats:\n",
    "        sc = MyMinMax(axis=1)\n",
    "        XN = sc.fit_transform(df.values)\n",
    "        scalersdict['allfeat'] = sc\n",
    "    else:\n",
    "        morph_feats = ['area', 'curv', 'thickness', 'volume']\n",
    "        XN = np.array([], dtype=np.double)\n",
    "        for ind, morph_feat in enumerate(morph_feats):\n",
    "            morph_cols = [col for col in df.columns if morph_feat in col]\n",
    "            X_morph = df.loc[:, morph_cols].values\n",
    "            Xn = (X_morph-np.min(X_morph, axis=1).reshape(-1,1))/(np.max(X_morph, axis=1).reshape(-1,1)-np.min(X_morph, axis=1).reshape(-1,1))\n",
    "            if ind == 0:\n",
    "                XN = np.append(XN, Xn).reshape(Xn.shape[0], -1)\n",
    "            else:\n",
    "                XN = np.concatenate([XN, Xn], axis=1)\n",
    "    return XN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal of this notebook:\n",
    "For each normalization method:<br>\n",
    "$\\;\\;\\;\\;\\;$ For each RFE classifier core:<br>\n",
    "$\\;\\;\\;\\;\\;$ $\\;\\;\\;\\;\\;$ For each data matrix (corr; uncorr; ucorrleft; ucorrright):<br>\n",
    "$\\;\\;\\;\\;\\;$ $\\;\\;\\;\\;\\;$ $\\;\\;\\;\\;\\;$ 1. Find the classifier with highest performance <br>\n",
    "$\\;\\;\\;\\;\\;$ $\\;\\;\\;\\;\\;$ $\\;\\;\\;\\;\\;$ 2. Use this classifier to train on all the training set<br>\n",
    "$\\;\\;\\;\\;\\;$ $\\;\\;\\;\\;\\;$ $\\;\\;\\;\\;\\;$ 3. Measure the performance on the testing set<br><br>\n",
    "\n",
    "Measure the performance in the testing set:\n",
    "1. Load the testing set\n",
    "2. Get the normalization object corresponding to the current normalization method\n",
    "3. Normalize the testing set using the normalization object of the training set\n",
    "4. Load the rfe+(RFE classifier core)\n",
    "5. Get the selected features used for learning the best ML model\n",
    "6. Select those features out of the normalized testing set\n",
    "7. Predict the labels of the output matrix from step 6\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression l1-norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = load('./Final_Results_DP/ML/clf_lg1_train.joblib')\n",
    "clf_corr = load('./Final_Results_DP/ML/clf_lg1_train_corr.joblib')\n",
    "clf_corr_l = load('./Final_Results_DP/ML/clf_lg1_train_corr_l.joblib')\n",
    "clf_corr_r = load('./Final_Results_DP/ML/clf_lg1_train_corr_r.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf_lg1_train\n",
      "0.6776490132635875\n",
      "0.6583750519308025\n",
      "0.6761797859734646\n",
      "0.5884697143652369\n",
      "0.6825626188268857\n",
      "0.6153220016740648\n",
      "0000000000000000000000000000000000000000000\n",
      "clf_lg1_train_corr\n",
      "0.6607855314132751\n",
      "0.6393951263837128\n",
      "0.656329172729524\n",
      "0.5933545354265284\n",
      "0.6546790011276403\n",
      "0.5929751489365186\n",
      "0000000000000000000000000000000000000000000\n",
      "clf_lg1_train_corr_l\n",
      "0.6260678917219742\n",
      "0.6100080428789824\n",
      "0.6231945885545535\n",
      "0.6025791343570097\n",
      "0.6460843663214163\n",
      "0.6095766948883717\n",
      "0000000000000000000000000000000000000000000\n",
      "clf_lg1_train_corr_r\n",
      "0.5971593185860087\n",
      "0.5740063974655721\n",
      "0.5957127054976045\n",
      "0.5786779185111056\n",
      "0.5948770646970822\n",
      "0.5824622056316526\n"
     ]
    }
   ],
   "source": [
    "print(\"clf_lg1_train\")\n",
    "print(clf['lSVM'].best_score_)\n",
    "print(clf['pagg'].best_score_)\n",
    "print(clf['lg'].best_score_)\n",
    "print(clf['GNB'].best_score_)\n",
    "print(clf['SVC'].best_score_)\n",
    "print(clf['Rf'].best_score_)\n",
    "print(\"0000000000000000000000000000000000000000000\")\n",
    "print(\"clf_lg1_train_corr\")\n",
    "print(clf_corr['lSVM'].best_score_)\n",
    "print(clf_corr['pagg'].best_score_)\n",
    "print(clf_corr['lg'].best_score_)\n",
    "print(clf_corr['GNB'].best_score_)\n",
    "print(clf_corr['SVC'].best_score_)\n",
    "print(clf_corr['Rf'].best_score_)\n",
    "print(\"0000000000000000000000000000000000000000000\")\n",
    "print(\"clf_lg1_train_corr_l\")\n",
    "print(clf_corr_l['lSVM'].best_score_)\n",
    "print(clf_corr_l['pagg'].best_score_)\n",
    "print(clf_corr_l['lg'].best_score_)\n",
    "print(clf_corr_l['GNB'].best_score_)\n",
    "print(clf_corr_l['SVC'].best_score_)\n",
    "print(clf_corr_l['Rf'].best_score_)\n",
    "print(\"0000000000000000000000000000000000000000000\")\n",
    "print(\"clf_lg1_train_corr_r\")\n",
    "print(clf_corr_r['lSVM'].best_score_)\n",
    "print(clf_corr_r['pagg'].best_score_)\n",
    "print(clf_corr_r['lg'].best_score_)\n",
    "print(clf_corr_r['GNB'].best_score_)\n",
    "print(clf_corr_r['SVC'].best_score_)\n",
    "print(clf_corr_r['Rf'].best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the current results, I am going to proceed with \"lg1_train_corr\", classifier XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected classifier 1: lSVM_alldata with accuracy 0.6776490132635875\n",
      "selected classifier 2: lg_alldata with accuracy 0.6761797859734646\n",
      "selected classifier 3: SVC_alldata with accuracy 0.6825626188268857\n",
      "selected classifier 4: lSVM_alldata with accuracy 0.6607855314132751\n",
      "selected classifier 5: lg_alldata with accuracy 0.656329172729524\n",
      "selected classifier 6: SVC_alldata with accuracy 0.6546790011276403\n",
      "selected classifier 7: lSVM_alldata with accuracy 0.6260678917219742\n",
      "selected classifier 8: lg_alldata with accuracy 0.6460843663214163\n"
     ]
    }
   ],
   "source": [
    "selected_clc1 = clf['lSVM'].best_estimator_\n",
    "selected_clc2 = clf['lg'].best_estimator_\n",
    "selected_clc3 = clf['SVC'].best_estimator_\n",
    "\n",
    "selected_clc4 = clf_corr['lSVM'].best_estimator_\n",
    "selected_clc5 = clf_corr['lg'].best_estimator_\n",
    "selected_clc6 = clf_corr['SVC'].best_estimator_\n",
    "\n",
    "selected_clc7 = clf_corr_l['lSVM'].best_estimator_\n",
    "selected_clc8 = clf_corr_l['SVC'].best_estimator_\n",
    "\n",
    "\n",
    "print(f\"selected classifier 1: lSVM_alldata with accuracy {clf['lSVM'].best_score_}\")\n",
    "print(f\"selected classifier 2: lg_alldata with accuracy {clf['lg'].best_score_}\")\n",
    "print(f\"selected classifier 3: SVC_alldata with accuracy {clf['SVC'].best_score_}\")\n",
    "print(f\"selected classifier 4: lSVM_alldata with accuracy {clf_corr['lSVM'].best_score_}\")\n",
    "print(f\"selected classifier 5: lg_alldata with accuracy {clf_corr['lg'].best_score_}\")\n",
    "print(f\"selected classifier 6: SVC_alldata with accuracy {clf_corr['SVC'].best_score_}\")\n",
    "print(f\"selected classifier 7: lSVM_alldata with accuracy {clf_corr_l['lSVM'].best_score_}\")\n",
    "print(f\"selected classifier 8: lg_alldata with accuracy {clf_corr_l['SVC'].best_score_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clf['SVC'].best_params_, clf['SVC'].n_features_in_, clf['SVC'].best_score_\n",
    "clf['GNB'].n_features_in_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67, 545)\n",
      "0    36\n",
      "1    31\n",
      "Name: labels, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load test dataset\n",
    "df_test = pd.read_csv('./Final_Results_DP/INITIAL_SPLIT/test_fullbrain.csv', index_col=0)\n",
    "print(df_test.shape)\n",
    "print(df_test['labels'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline score:  0.5373134328358209\n"
     ]
    }
   ],
   "source": [
    "print('baseline score: ',36/(31+36))\n",
    "XN = mynormalize(df_test, allfeats=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67, 11)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the corresponding rfe object\n",
    "selected_rfe1 = load('./Final_Results/FS/rfetrain_corr_lg1.joblib')\n",
    "Xtest = XN[:, np.where(selected_rfe1.support_)[0]]\n",
    "Xtest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((597, 11), (597,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load training dataset to train the current model using all training set\n",
    "Xtrain = np.load('./Final_Results/FS/Xtrain_corr_lg1.npy')\n",
    "ytrain = np.load('./Final_Results/FS/ytrain_corr.npy')\n",
    "Xtrain.shape, ytrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      1.00      0.70        36\n",
      "           1       0.00      0.00      0.00        31\n",
      "\n",
      "    accuracy                           0.54        67\n",
      "   macro avg       0.27      0.50      0.35        67\n",
      "weighted avg       0.29      0.54      0.38        67\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\untitled\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df_test['labels'].values, selected_clc.predict(Xtest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.82      0.82       300\n",
      "           1       0.82      0.80      0.81       297\n",
      "\n",
      "    accuracy                           0.81       597\n",
      "   macro avg       0.81      0.81      0.81       597\n",
      "weighted avg       0.81      0.81      0.81       597\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected_clc = selected_clc.fit(Xtrain, ytrain)\n",
    "print(classification_report(ytrain, selected_clc.predict(Xtrain)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.64      0.58        36\n",
      "           1       0.46      0.35      0.40        31\n",
      "\n",
      "    accuracy                           0.51        67\n",
      "   macro avg       0.50      0.50      0.49        67\n",
      "weighted avg       0.50      0.51      0.50        67\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df_test['labels'].values, selected_clc.predict(Xtest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression l2-norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = load('./Final_Results_DP/ML/clf_lg2_train.joblib')\n",
    "clf_corr = load('./Final_Results_DP/ML/clf_lg2_train_corr.joblib')\n",
    "clf_corr_l = load('./Final_Results_DP/ML/clf_lg2_train_corr_l.joblib')\n",
    "clf_corr_r = load('./Final_Results_DP/ML/clf_lg2_train_corr_r.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf_lg2_train\n",
      "0.6718001927835116\n",
      "0.6411071647112033\n",
      "0.6748771158604346\n",
      "0.5954639799767104\n",
      "0.6777237526886342\n",
      "0.6020947504354001\n",
      "0000000000000000000000000000000000000000000\n",
      "clf_lg2_train_corr\n",
      "0.6442305441120191\n",
      "0.6057018997975978\n",
      "0.6472834613440408\n",
      "0.5502858394166559\n",
      "0.6516473064541546\n",
      "0.5839528089703683\n",
      "0000000000000000000000000000000000000000000\n",
      "clf_lg2_train_corr_l\n",
      "0.6212269995149715\n",
      "0.5898393061430813\n",
      "0.6212749805067628\n",
      "0.6084847666234848\n",
      "0.6383680527842073\n",
      "0.6007139538834008\n",
      "0000000000000000000000000000000000000000000\n",
      "clf_lg2_train_corr_r\n",
      "0.6320134007052356\n",
      "0.582801735870217\n",
      "0.6305428124699415\n",
      "0.5459638970920797\n",
      "0.6407877109720832\n",
      "0.5927770342037242\n"
     ]
    }
   ],
   "source": [
    "print(\"clf_lg2_train\")\n",
    "print(clf['lSVM'].best_score_)\n",
    "print(clf['pagg'].best_score_)\n",
    "print(clf['lg'].best_score_)\n",
    "print(clf['GNB'].best_score_)\n",
    "print(clf['SVC'].best_score_)\n",
    "print(clf['Rf'].best_score_)\n",
    "print(\"0000000000000000000000000000000000000000000\")\n",
    "print(\"clf_lg2_train_corr\")\n",
    "print(clf_corr['lSVM'].best_score_)\n",
    "print(clf_corr['pagg'].best_score_)\n",
    "print(clf_corr['lg'].best_score_)\n",
    "print(clf_corr['GNB'].best_score_)\n",
    "print(clf_corr['SVC'].best_score_)\n",
    "print(clf_corr['Rf'].best_score_)\n",
    "print(\"0000000000000000000000000000000000000000000\")\n",
    "print(\"clf_lg2_train_corr_l\")\n",
    "print(clf_corr_l['lSVM'].best_score_)\n",
    "print(clf_corr_l['pagg'].best_score_)\n",
    "print(clf_corr_l['lg'].best_score_)\n",
    "print(clf_corr_l['GNB'].best_score_)\n",
    "print(clf_corr_l['SVC'].best_score_)\n",
    "print(clf_corr_l['Rf'].best_score_)\n",
    "print(\"0000000000000000000000000000000000000000000\")\n",
    "print(\"clf_lg2_train_corr_r\")\n",
    "print(clf_corr_r['lSVM'].best_score_)\n",
    "print(clf_corr_r['pagg'].best_score_)\n",
    "print(clf_corr_r['lg'].best_score_)\n",
    "print(clf_corr_r['GNB'].best_score_)\n",
    "print(clf_corr_r['SVC'].best_score_)\n",
    "print(clf_corr_r['Rf'].best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected classifier 1: lSVM_alldata with accuracy 0.6535310734463277\n",
      "selected classifier 2: SVC_alldata with accuracy 0.6450564971751412\n",
      "selected classifier 3: lSVM_corr with accuracy 0.6231920903954802\n"
     ]
    }
   ],
   "source": [
    "selected_clc1 = clf['lSVM'].best_estimator_\n",
    "selected_clc2 = clf['SVC'].best_estimator_\n",
    "selected_clc3 = clf_corr['lSVM'].best_estimator_\n",
    "print(f'selected classifier 1: lSVM_alldata with accuracy {clf[\"lSVM\"].best_score_}')\n",
    "print(f'selected classifier 2: SVC_alldata with accuracy {clf[\"SVC\"].best_score_}')\n",
    "print(f'selected classifier 3: lSVM_corr with accuracy {clf_corr[\"lSVM\"].best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67, 545)\n",
      "0    36\n",
      "1    31\n",
      "Name: labels, dtype: int64\n",
      "baseline score:  0.5373134328358209\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(67, 11)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load test dataset\n",
    "df_test = pd.read_csv('./Final_Results/INITIAL_SPLIT/test_fullbrain.csv', index_col=0)\n",
    "print(df_test.shape)\n",
    "print(df_test['labels'].value_counts())\n",
    "\n",
    "print('baseline score: ',36/(31+36))\n",
    "XN = mynormalize(df_test, allfeats=False)\n",
    "\n",
    "# Load the corresponding rfe object\n",
    "#clc1 & 2\n",
    "selected_rfe12 = load('./Final_Results/FS/rfetrain_lg2.joblib')\n",
    "Xtest12 = XN[:, np.where(selected_rfe12.support_)[0]]\n",
    "\n",
    "# clc3\n",
    "selected_rfe3 = load('./Final_Results/FS/rfetrain_corr_lg2.joblib')\n",
    "Xtest3 = XN[:, np.where(selected_rfe3.support_)[0]]\n",
    "Xtest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training dataset to train the current model using all training set\n",
    "# clc1&2\n",
    "Xtrain12 = np.load('./Final_Results/FS/Xtrain_lg2.npy')\n",
    "\n",
    "# clc3\n",
    "Xtrain3 = np.load('./Final_Results/FS/Xtrain_corr_lg2.npy')\n",
    "\n",
    "ytrain = np.load('./Final_Results/FS/ytrain_corr.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.64      0.61        36\n",
      "           1       0.54      0.48      0.51        31\n",
      "\n",
      "    accuracy                           0.57        67\n",
      "   macro avg       0.56      0.56      0.56        67\n",
      "weighted avg       0.56      0.57      0.56        67\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# clc1\n",
    "print(classification_report(df_test['labels'].values, selected_clc1.predict(Xtest12)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.61      0.59        36\n",
      "           1       0.50      0.45      0.47        31\n",
      "\n",
      "    accuracy                           0.54        67\n",
      "   macro avg       0.53      0.53      0.53        67\n",
      "weighted avg       0.53      0.54      0.53        67\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# clc2\n",
    "print(classification_report(df_test['labels'].values, selected_clc2.predict(Xtest12)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.97      0.71        36\n",
      "           1       0.75      0.10      0.17        31\n",
      "\n",
      "    accuracy                           0.57        67\n",
      "   macro avg       0.65      0.53      0.44        67\n",
      "weighted avg       0.65      0.57      0.46        67\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# clc3\n",
    "print(classification_report(df_test['labels'].values, selected_clc3.predict(Xtest3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_clc1 = selected_clc1.fit(Xtrain12, ytrain)\n",
    "selected_clc2 = selected_clc2.fit(Xtrain12, ytrain)\n",
    "selected_clc3 = selected_clc3.fit(Xtrain3, ytrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.64      0.61        36\n",
      "           1       0.54      0.48      0.51        31\n",
      "\n",
      "    accuracy                           0.57        67\n",
      "   macro avg       0.56      0.56      0.56        67\n",
      "weighted avg       0.56      0.57      0.56        67\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# clc1\n",
    "print(classification_report(df_test['labels'].values, selected_clc1.predict(Xtest12)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.61      0.59        36\n",
      "           1       0.50      0.45      0.47        31\n",
      "\n",
      "    accuracy                           0.54        67\n",
      "   macro avg       0.53      0.53      0.53        67\n",
      "weighted avg       0.53      0.54      0.53        67\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# clc2\n",
    "print(classification_report(df_test['labels'].values, selected_clc2.predict(Xtest12)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.97      0.71        36\n",
      "           1       0.75      0.10      0.17        31\n",
      "\n",
      "    accuracy                           0.57        67\n",
      "   macro avg       0.65      0.53      0.44        67\n",
      "weighted avg       0.65      0.57      0.46        67\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# clc3\n",
    "print(classification_report(df_test['labels'].values, selected_clc3.predict(Xtest3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = load('./Final_Results_DP/ML/clf_svm_train.joblib')\n",
    "clf_corr = load('./Final_Results_DP/ML/clf_svm_train_corr.joblib')\n",
    "clf_corr_l = load('./Final_Results_DP/ML/clf_svm_train_corr_l.joblib')\n",
    "clf_corr_r = load('./Final_Results_DP/ML/clf_svm_train_corr_r.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf_svm_train\n",
      "0.6183756454256895\n",
      "0.6179362136254147\n",
      "0.6140563942934444\n",
      "0.5833434329790783\n",
      "0.6199826453908983\n",
      "0.6022306198337806\n",
      "0000000000000000000000000000000000000000000\n",
      "clf_svm_train_corr\n",
      "0.667025086414902\n",
      "0.5953776264706905\n",
      "0.6671169348649595\n",
      "0.5590827024891994\n",
      "0.673043155264402\n",
      "0.5956003303106024\n",
      "0000000000000000000000000000000000000000000\n",
      "clf_svm_train_corr_l\n",
      "0.6248221766527298\n",
      "0.6175906870419601\n",
      "0.6232378215872509\n",
      "0.6011620322902148\n",
      "0.6326042760283321\n",
      "0.6009642245375344\n",
      "0000000000000000000000000000000000000000000\n",
      "clf_svm_train_corr_r\n",
      "0.5617032484635646\n",
      "0.5588516180921801\n",
      "0.5667843014462857\n",
      "0.5545365828201649\n",
      "0.5840966598517081\n",
      "0.5622388162028197\n"
     ]
    }
   ],
   "source": [
    "print(\"clf_svm_train\")\n",
    "print(clf['lSVM'].best_score_)\n",
    "print(clf['pagg'].best_score_)\n",
    "print(clf['lg'].best_score_)\n",
    "print(clf['GNB'].best_score_)\n",
    "print(clf['SVC'].best_score_)\n",
    "print(clf['Rf'].best_score_)\n",
    "print(\"0000000000000000000000000000000000000000000\")\n",
    "print(\"clf_svm_train_corr\")\n",
    "print(clf_corr['lSVM'].best_score_)\n",
    "print(clf_corr['pagg'].best_score_)\n",
    "print(clf_corr['lg'].best_score_)\n",
    "print(clf_corr['GNB'].best_score_)\n",
    "print(clf_corr['SVC'].best_score_)\n",
    "print(clf_corr['Rf'].best_score_)\n",
    "print(\"0000000000000000000000000000000000000000000\")\n",
    "print(\"clf_svm_train_corr_l\")\n",
    "print(clf_corr_l['lSVM'].best_score_)\n",
    "print(clf_corr_l['pagg'].best_score_)\n",
    "print(clf_corr_l['lg'].best_score_)\n",
    "print(clf_corr_l['GNB'].best_score_)\n",
    "print(clf_corr_l['SVC'].best_score_)\n",
    "print(clf_corr_l['Rf'].best_score_)\n",
    "print(\"0000000000000000000000000000000000000000000\")\n",
    "print(\"clf_svm_train_corr_r\")\n",
    "print(clf_corr_r['lSVM'].best_score_)\n",
    "print(clf_corr_r['pagg'].best_score_)\n",
    "print(clf_corr_r['lg'].best_score_)\n",
    "print(clf_corr_r['GNB'].best_score_)\n",
    "print(clf_corr_r['SVC'].best_score_)\n",
    "print(clf_corr_r['Rf'].best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'solver': 'newton-cg', 'penalty': 'none', 'C': 0.1}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_corr['lg'].best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected classifier 1: SVC_alldata with accuracy 0.651723163841808\n",
      "selected classifier 2: SVC_corr with accuracy 0.6518079096045198\n",
      "selected classifier 3: lg_corr with accuracy 0.6466666666666667\n"
     ]
    }
   ],
   "source": [
    "selected_clc1 = clf['SVC'].best_estimator_\n",
    "selected_clc2 = clf_corr['SVC'].best_estimator_\n",
    "selected_clc3 = clf_corr['lg'].best_estimator_\n",
    "print(f\"selected classifier 1: SVC_alldata with accuracy {clf['SVC'].best_score_}\")\n",
    "print(f\"selected classifier 2: SVC_corr with accuracy {clf_corr['SVC'].best_score_}\")\n",
    "print(f\"selected classifier 3: lg_corr with accuracy {clf_corr['lg'].best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67, 545)\n",
      "0    36\n",
      "1    31\n",
      "Name: labels, dtype: int64\n",
      "baseline score:  0.5373134328358209\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(67, 11)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load test dataset\n",
    "df_test = pd.read_csv('./Final_Results/INITIAL_SPLIT/test_fullbrain.csv', index_col=0)\n",
    "print(df_test.shape)\n",
    "print(df_test['labels'].value_counts())\n",
    "\n",
    "print('baseline score: ',36/(31+36))\n",
    "XN = mynormalize(df_test, allfeats=False)\n",
    "\n",
    "# Load the corresponding rfe object\n",
    "#clc1\n",
    "selected_rfe1 = load('./Final_Results/FS/rfetrain_svm.joblib')\n",
    "Xtest1 = XN[:, np.where(selected_rfe1.support_)[0]]\n",
    "\n",
    "# clc2,3\n",
    "selected_rfe23 = load('./Final_Results/FS/rfetrain_corr_svm.joblib')\n",
    "Xtest23 = XN[:, np.where(selected_rfe23.support_)[0]]\n",
    "Xtest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training dataset to train the current model using all training set\n",
    "# clc1\n",
    "Xtrain1 = np.load('./Final_Results/FS/Xtrain_svm.npy')\n",
    "\n",
    "# clc23\n",
    "Xtrain23 = np.load('./Final_Results/FS/Xtrain_corr_svm.npy')\n",
    "\n",
    "ytrain = np.load('./Final_Results/FS/ytrain_corr.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.53      0.56        36\n",
      "           1       0.51      0.58      0.55        31\n",
      "\n",
      "    accuracy                           0.55        67\n",
      "   macro avg       0.55      0.55      0.55        67\n",
      "weighted avg       0.56      0.55      0.55        67\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# clc1\n",
    "print(classification_report(df_test['labels'].values, selected_clc1.predict(Xtest1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        36\n",
      "           1       0.46      1.00      0.63        31\n",
      "\n",
      "    accuracy                           0.46        67\n",
      "   macro avg       0.23      0.50      0.32        67\n",
      "weighted avg       0.21      0.46      0.29        67\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\untitled\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# clc2\n",
    "print(classification_report(df_test['labels'].values, selected_clc2.predict(Xtest23)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.81      0.68        36\n",
      "           1       0.61      0.35      0.45        31\n",
      "\n",
      "    accuracy                           0.60        67\n",
      "   macro avg       0.60      0.58      0.57        67\n",
      "weighted avg       0.60      0.60      0.57        67\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# clc3\n",
    "print(classification_report(df_test['labels'].values, selected_clc3.predict(Xtest23)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\untitled\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    }
   ],
   "source": [
    "selected_clc1 = selected_clc1.fit(Xtrain1, ytrain)\n",
    "selected_clc2 = selected_clc2.fit(Xtrain23, ytrain)\n",
    "selected_clc3 = selected_clc3.fit(Xtrain23, ytrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.53      0.56        36\n",
      "           1       0.51      0.58      0.55        31\n",
      "\n",
      "    accuracy                           0.55        67\n",
      "   macro avg       0.55      0.55      0.55        67\n",
      "weighted avg       0.56      0.55      0.55        67\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# clc1\n",
    "print(classification_report(df_test['labels'].values, selected_clc1.predict(Xtest1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        36\n",
      "           1       0.46      1.00      0.63        31\n",
      "\n",
      "    accuracy                           0.46        67\n",
      "   macro avg       0.23      0.50      0.32        67\n",
      "weighted avg       0.21      0.46      0.29        67\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\untitled\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# clc2\n",
    "print(classification_report(df_test['labels'].values, selected_clc2.predict(Xtest23)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.81      0.68        36\n",
      "           1       0.61      0.35      0.45        31\n",
      "\n",
      "    accuracy                           0.60        67\n",
      "   macro avg       0.60      0.58      0.57        67\n",
      "weighted avg       0.60      0.60      0.57        67\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# clc3\n",
    "print(classification_report(df_test['labels'].values, selected_clc3.predict(Xtest23)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = load('./Final_Results_DP/ML/clf__rf_train.joblib')\n",
    "clf_corr = load('./Final_Results_DP/ML/clf__rf_train_corr.joblib')\n",
    "clf_corr_l = load('./Final_Results_DP/ML/clf__rf_train_corr_l.joblib')\n",
    "clf_corr_r = load('./Final_Results_DP/ML/clf__rf_train_corr_r.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf__rf_train\n",
      "0.6423945574472352\n",
      "0.592404053365423\n",
      "0.6380067678882428\n",
      "0.5548148193626684\n",
      "0.6421218260814398\n",
      "0.6077900194625392\n",
      "0000000000000000000000000000000000000000000\n",
      "clf__rf_train_corr\n",
      "0.5732096101147901\n",
      "0.553707521626749\n",
      "0.5972072688797885\n",
      "0.5437919923214041\n",
      "0.5885222693607242\n",
      "0.5965520300594926\n",
      "0000000000000000000000000000000000000000000\n",
      "clf__rf_train_corr_l\n",
      "0.6032624925557322\n",
      "0.5747999717578296\n",
      "0.6018604427471855\n",
      "0.5289584676371331\n",
      "0.6126722414255338\n",
      "0.5954385210926036\n",
      "0000000000000000000000000000000000000000000\n",
      "clf__rf_train_corr_r\n",
      "0.5752937492709223\n",
      "0.5450725803315795\n",
      "0.5751799517427261\n",
      "0.5577725011307101\n",
      "0.5947266956046587\n",
      "0.568341294350952\n"
     ]
    }
   ],
   "source": [
    "print(\"clf__rf_train\")\n",
    "print(clf['lSVM'].best_score_)\n",
    "print(clf['pagg'].best_score_)\n",
    "print(clf['lg'].best_score_)\n",
    "print(clf['GNB'].best_score_)\n",
    "print(clf['SVC'].best_score_)\n",
    "print(clf['Rf'].best_score_)\n",
    "print(\"0000000000000000000000000000000000000000000\")\n",
    "print(\"clf__rf_train_corr\")\n",
    "print(clf_corr['lSVM'].best_score_)\n",
    "print(clf_corr['pagg'].best_score_)\n",
    "print(clf_corr['lg'].best_score_)\n",
    "print(clf_corr['GNB'].best_score_)\n",
    "print(clf_corr['SVC'].best_score_)\n",
    "print(clf_corr['Rf'].best_score_)\n",
    "print(\"0000000000000000000000000000000000000000000\")\n",
    "print(\"clf__rf_train_corr_l\")\n",
    "print(clf_corr_l['lSVM'].best_score_)\n",
    "print(clf_corr_l['pagg'].best_score_)\n",
    "print(clf_corr_l['lg'].best_score_)\n",
    "print(clf_corr_l['GNB'].best_score_)\n",
    "print(clf_corr_l['SVC'].best_score_)\n",
    "print(clf_corr_l['Rf'].best_score_)\n",
    "print(\"0000000000000000000000000000000000000000000\")\n",
    "print(\"clf__rf_train_corr_r\")\n",
    "print(clf_corr_r['lSVM'].best_score_)\n",
    "print(clf_corr_r['pagg'].best_score_)\n",
    "print(clf_corr_r['lg'].best_score_)\n",
    "print(clf_corr_r['GNB'].best_score_)\n",
    "print(clf_corr_r['SVC'].best_score_)\n",
    "print(clf_corr_r['Rf'].best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected classifier 1: SVC_alldata with accuracy 0.6080508474576272\n",
      "selected classifier 2: SVC_corr with accuracy 0.601412429378531\n"
     ]
    }
   ],
   "source": [
    "selected_clc1 = clf_corr['lg'].best_estimator_\n",
    "selected_clc2 = clf_corr['nn'].best_estimator_\n",
    "print(f\"selected classifier 1: SVC_alldata with accuracy {clf_corr['lg'].best_score_}\")\n",
    "print(f\"selected classifier 2: SVC_corr with accuracy {clf_corr['nn'].best_score_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67, 545)\n",
      "0    36\n",
      "1    31\n",
      "Name: labels, dtype: int64\n",
      "baseline score:  0.5373134328358209\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(67, 11)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load test dataset\n",
    "df_test = pd.read_csv('./Final_Results/INITIAL_SPLIT/test_fullbrain.csv', index_col=0)\n",
    "print(df_test.shape)\n",
    "print(df_test['labels'].value_counts())\n",
    "\n",
    "print('baseline score: ',36/(31+36))\n",
    "XN = mynormalize(df_test, allfeats=False)\n",
    "\n",
    "# Load the corresponding rfe object\n",
    "#clc1\n",
    "selected_rfe1 = load('./Final_Results/FS/rfetrain_corr_rf.joblib')\n",
    "Xtest1 = XN[:, np.where(selected_rfe1.support_)[0]]\n",
    "\n",
    "# clc2,3\n",
    "selected_rfe2 = load('./Final_Results/FS/rfetrain_corr_rf.joblib')\n",
    "Xtest2 = XN[:, np.where(selected_rfe2.support_)[0]]\n",
    "Xtest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training dataset to train the current model using all training set\n",
    "# clc1\n",
    "Xtrain1 = np.load('./Final_Results/FS/Xtrain_corr_rf.npy')\n",
    "ytrain = np.load('./Final_Results/FS/ytrain_corr.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.89      0.69        36\n",
      "           1       0.60      0.19      0.29        31\n",
      "\n",
      "    accuracy                           0.57        67\n",
      "   macro avg       0.58      0.54      0.49        67\n",
      "weighted avg       0.58      0.57      0.51        67\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# clc1\n",
    "print(classification_report(df_test['labels'].values, selected_clc1.predict(Xtest1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        36\n",
      "           1       0.46      1.00      0.63        31\n",
      "\n",
      "    accuracy                           0.46        67\n",
      "   macro avg       0.23      0.50      0.32        67\n",
      "weighted avg       0.21      0.46      0.29        67\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\untitled\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# clc2\n",
    "print(classification_report(df_test['labels'].values, selected_clc2.predict(Xtest2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_clc1 = selected_clc1.fit(Xtrain1, ytrain)\n",
    "selected_clc2 = selected_clc2.fit(Xtrain1, ytrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.89      0.69        36\n",
      "           1       0.60      0.19      0.29        31\n",
      "\n",
      "    accuracy                           0.57        67\n",
      "   macro avg       0.58      0.54      0.49        67\n",
      "weighted avg       0.58      0.57      0.51        67\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# clc1\n",
    "print(classification_report(df_test['labels'].values, selected_clc1.predict(Xtest1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        36\n",
      "           1       0.46      1.00      0.63        31\n",
      "\n",
      "    accuracy                           0.46        67\n",
      "   macro avg       0.23      0.50      0.32        67\n",
      "weighted avg       0.21      0.46      0.29        67\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\untitled\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# clc2\n",
    "print(classification_report(df_test['labels'].values, selected_clc2.predict(Xtest2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "untitled",
   "language": "python",
   "name": "untitled"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
