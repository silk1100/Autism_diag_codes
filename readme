
This branch uses the same pipeline for different feature representation:
Instead of median and IQR for each morphological feature of every brain region,
I use median-IQR and median+IQR for each morphological feature of every brain region

** TODO
* Split the brain into 2 hemispheres (checked)
* For each hemisphere apply correlation analysis (checked)
* Perform RF Feature selection
* Evaluate the selected features generalizability on various models
* Compare the resulted model by the output of pycaret on the raw data without any feature
  engineering.

** TODO 2
-- Look at the PCA components of LEFT AND RIGHT hemispheres to see if they are good representations (Done, PCA_components_test_SIDEEXP.py)
-- Update the data matrix to be median-IQR&median+IQR for every feat-brain region instead of median&IQR
* Given:
    -- Should I cluster based on age, gender, and site here first?? Maybe include aspergas and Nos-
    - Training and testing data matrices split into right and left semi-hemisphere in ./Results/INITIAL_SPLIT
    -- Should I split the training set training-validation sets here as well??
    - Removing correlated features from the training matrix (Left, right) in ./Results/CORR_ANA
    -- I didn't normalize the features before I feed them to SVM --
    - RFECV-SVM, RFECV-RF on each uncorrelated feature resulted left_SVM, right_SVM, left_RF, right_RF
    -- How different are the selected features from SVM and RF, and from LEFT AND RIGHT? (Done, answer in
        scores_dict_analysis.ipynb)
    -


** Observations
* What if I applied Hierarchical Feature selection but instead of using obj.ranking_, I will use the positions in
obj.grid_scores_ at which the accuracy increases as the index of features that when accumulated they improve the
model. Create out of those features a new data matrix and fed it one more time to the feature selector algorithm and
observe if the number of positions at which an increase in accuracy occurs increase or not. (wrong observation,
because obj_grid_scores_ is mapped to the number of features used not for the indix of the used features).



** Some cool notes:
* Multicollinearity:
(Assuming you are talking about supervised learning)
Correlated features will not always worsen your model, but they will not always improve it either.
There are three main reasons why you would remove correlated features:
    Make the learning algorithm faster
Due to the curse of dimensionality, less features usually mean high improvement in terms of speed.
If speed is not an issue, perhaps don't remove these features right away (see next point)
    Decrease harmful bias
The keyword being harmful. If you have correlated features but they are also correlated to the target,
you want to keep them. You can view features as hints to make a good guess, if you have two hints that are
essentially the same, but they are good hints, it may be wise to keep them.
Some algorithms like Naive Bayes actually directly benefit from "positive" correlated features.And others like
random forest may indirectly benefit from them.

Imagine having 3 features A, B, and C. A and B are highly correlated to the target and to each other,
and C isn't at all. If you sample out of the 3 features, you have 2/3 chance to get a "good" feature, whereas
if you remove B for instance, this chance drops to 1/2.

Of course, if the features that are correlated are not super informative in the first place,the algorithm may
not suffer much. So moral of the story, removing these features might be necessary due to speed, but remember that
you might make your algorithm worse in the process. Also, some algorithms like decision trees have feature selection
embedded in them. A good way to deal with this is to use a wrapper method for feature selection. It will remove
redundant features only if they do not contribute directly to the performance. If they are useful like in naive
bayes, they will be kept. (Though remember that wrapper methods are expensive and may lead to overfitting)
    Interpretability of your model
If your model needs to be interpretable, you might be forced to make it simpler. Make sure to also remember Occam's
razor. If your model is not "that much" worse with less features, then you should probably use less features.
